Each generator and discriminator is a simplest possible fully connected network with one hidden layer and leaky ReLU activations. Precise layer dimensions are given in table 1. We train in two stages First, we train 100 MNIST GANs for 25 epochs each, saving a snapshot of the weights at epochs 5-25. In the second phase, we treat each of these 20*100 parameter vector snapshots as training examples in order to train a GANGAN. To test performance, we sample parameter vectors from the GANGAN and recast them as GANs. We then observe image quality of the sampled GANs. Not only do the resulting digits resemble the quality of those in the training set, but we can actually interpolate smoothly through training history. That is, the latent space of the GANGAN roughly orders GANs by their quality. We visualize the learned results in figures 2-5.

GANs are notoriously difficult to train, and GANGANs are no different. An additional source of friction arises in this setting between the GAN and GANGAN. In particular, GANs benefit from a large number of parameters. However, the dimensionality of the GAN weights is the dimensionality of the input to the GANGAN. Having a large input dimension to the GANGAN results in rapid collapse as the discriminator loss immediately goes to zero. More data appears to help, but each data point is an expensive to obtain GAN snapshot. One way around this is to learn more complex but more complex architectures. For example, we experimented with a small variant of DCGAN. However, small perturbations the weights in the first layers can have a large impact on the output. This makes more complex network architectures difficult to model with the GANGAN. However, we only experimented with this at extremely small scale (a single GTX 1080TI) due to lack of hardware availability. We expect that simply training a larger batch of GANs would help alleviate this issue.


Conclusion: Can you do the gan gan? At least for small, simple networks: yes you gan!
